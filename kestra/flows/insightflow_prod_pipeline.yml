id: insightflow_prod_pipeline # Unique ID for the flow
namespace: prod.pipelines.insightflow # Namespace to organize flows

description: |
  End-to-end pipeline for ingesting data.gov.my retail and fuel data,
  processing it with dbt, and running tests. Targets the PRODUCTION environment.

# Optional: Define inputs for the flow, e.g., target dbt environment
inputs:
  - name: dbt_target
    type: STRING
    defaults: prod

tasks:
# -------------------------------------
# 1. Ingestion via AWS Batch (using AWS CLI)
# -------------------------------------
- id: submit_batch_ingestion_job_cli
  type: io.kestra.core.tasks.scripts.Bash
  commands:
    - |
      echo "Submitting AWS Batch Job..."
      JOB_DEF_NAME="insightflow-prod-ingestion-job-def"
      JOB_QUEUE_NAME="insightflow-prod-job-queue"
      TARGET_BUCKET_NAME="insightflow-prod-raw-data"
      AWS_REGION="ap-southeast-2"

      JOB_NAME="insightflow-ingestion-{{execution.id}}"
      JOB_OUTPUT=$(aws batch submit-job \
        --region "$AWS_REGION" \
        --job-name "$JOB_NAME" \
        --job-queue "$JOB_QUEUE_NAME" \
        --job-definition "$JOB_DEF_NAME" \
        --container-overrides '{
            "environment": [
              {"name": "TARGET_BUCKET", "value": "'"$TARGET_BUCKET_NAME"'"}
            ]
          }')

      JOB_ID=$(echo "$JOB_OUTPUT" | grep -o '"jobId": "[^"]*' | awk -F'"' '{print $4}')
      echo "Submitted Job ID: $JOB_ID"
      echo '\{\{ outputs({"jobId": "'"$JOB_ID"'"}) \}\}'

# -------------------------------------
# 2.1 Update Glue Catalog via Crawler (using AWS CLI)
# -------------------------------------
- id: start_glue_crawler_cli
  type: io.kestra.core.tasks.scripts.Bash
  commands:
    - |
      echo "Starting AWS Glue Crawler..."
      CRAWLER_NAME="insightflow-prod-raw-data-crawler"
      AWS_REGION="ap-southeast-2"

      aws glue start-crawler --region $AWS_REGION --name "$CRAWLER_NAME"
      echo "Crawler $CRAWLER_NAME started."

  # Similar to Batch, this doesn't wait for completion. Add delay or polling if needed.

# Add a delay mechanism to wait for the crawler's completion, 
# but note that 4 minutes (PT4M) is acceptable for this project's specific requirements.

# ***Determining the Delay Time:***
# 1. **Estimate the average crawl duration**: Go to **CloudWatch > Log groups > /aws-glue/crawlers > [crawler_name]** to review the logs and determine how long the crawler typically takes to complete its tasks.
# 2. **Consider the maximum allowed delay**: Determine how long the pipeline can afford to wait for the crawler to complete before failing or timing out.
# 3. **Calculate the safe delay time**: Use the estimated average crawl duration and maximum allowed delay to calculate a safe delay time that balances between waiting too long and failing too quickly.

# # --- Optional Delay ---
# - id: wait_for_crawler_and_batch
#   type: io.kestra.plugin.core.flow.Pause
#   delay: PT4M # Example: Pause for 4 minutes to give Batch/Crawler time

# # ***Always determine the delay time based on your project's unique needs and constraints.***

# -------------------------------------
# 2.2 Add a polling mechanism to wait for the crawler's completion
# -------------------------------------
- id: poll_glue_crawler
  type: io.kestra.core.tasks.scripts.Bash
  commands:
    - |
      echo "Polling AWS Glue Crawler for completion..."
      CRAWLER_NAME="insightflow-prod-raw-data-crawler"
      AWS_REGION="ap-southeast-2"
      MAX_RETRIES=10  # Maximum number of retries (e.g., 20 retries with 30-second intervals = 5 minutes)
      RETRY_COUNT=0

      while true; do
        STATUS=$(aws glue get-crawler --region "$AWS_REGION" --name "$CRAWLER_NAME" --query 'Crawler.State' --output text)
        echo "Current Crawler Status: $STATUS"

        if [ "$STATUS" = "READY" ]; then
          echo "Crawler $CRAWLER_NAME has completed successfully."
          break
        elif [ "$STATUS" = "RUNNING" ]; then
          echo "Crawler $CRAWLER_NAME is still running. Waiting for 30 seconds..."
        elif [ "$STATUS" = "STOPPING" ]; then
          echo "Crawler $CRAWLER_NAME is stopping. This might be a transient state. Waiting for 30 seconds..."
        else
          echo "Unexpected Crawler Status: $STATUS. Exiting with error."
          exit 1
        fi

        # Increment retry count and check if maximum retries have been reached
        RETRY_COUNT=$((RETRY_COUNT + 1))
        if [ "$RETRY_COUNT" -ge "$MAX_RETRIES" ]; then
          echo "Maximum retries reached. Crawler did not reach READY state. Exiting with error."
          exit 1
        fi

        sleep 30
      done

# -------------------------------------
# 3. Run dbt Tasks (Requires dbt CLI plugin & access to dbt project files)
# -------------------------------------
- id: dbt_setup_and_run
  type: io.kestra.plugin.core.flow.WorkingDirectory
  tasks:
  - id: sync_dbt_files
    type: io.kestra.plugin.git.SyncNamespaceFiles
    url: https://github.com/pizofreude/insightflow-retail-economic-pipeline
    branch: main
    namespace: "{{ flow.namespace }}"
    gitDirectory: dbt
    dryRun: false
    # disabled: true # Uncomment this after the first successful sync if you don't want to sync files every time

  - id: debug_synced_files
    type: io.kestra.core.tasks.scripts.Bash
    commands:
      - ls -l dbt

  - id: dbt_deps
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt deps
    namespaceFiles:
      enabled: true
    containerImage: pizofreude/kestra-dbt-athena:latest
    

  - id: dbt_seed
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt seed --target prod
    namespaceFiles:
      enabled: false  # Disable automatic file sync for this task
    profiles: |
      insightflow_dbt: # Profile name - must match profile in dbt_project.yml
        target: prod  # Tells dbt to use the 'prod' output block defined below
        outputs:
          prod:
            type: athena
            s3_staging_dir: "s3://insightflow-prod-processed-data/dbt-athena-results/"
            region_name: "ap-southeast-2"
            schema: "insightflow_prod"  # <<< ADD THIS: Target Glue DB name created by Terraform
            database: "awsdatacatalog" # <<< SET THIS: Use Glue Data Catalog alias
            threads: 4
            work_group: "primary"
    containerImage: pizofreude/kestra-dbt-athena:latest   

  - id: dbt_run
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt run --target prod
    namespaceFiles:
      enabled: false  # Disable automatic file sync for this task
    containerImage: pizofreude/kestra-dbt-athena:latest
    

  - id: dbt_test
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt test --target prod
    namespaceFiles:
      enabled: false  # Disable automatic file sync for this task
    containerImage: pizofreude/kestra-dbt-athena:latest

# Optional: Add triggers (e.g., schedule)
triggers:
  - id: daily_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 5 * * *"