id: insightflow_dev_pipeline # Unique ID for the flow
namespace: dev.pipelines.insightflow # Namespace to organize flows

description: |
  End-to-end pipeline for ingesting data.gov.my retail and fuel data,
  processing it with dbt, and running tests. Targets the DEV environment.

# Optional: Define inputs for the flow, e.g., target dbt environment
inputs:
  - name: dbt_target
    type: STRING
    defaults: dev

tasks:
# -------------------------------------
# 1. Ingestion via AWS Batch (using AWS CLI)
# -------------------------------------
- id: submit_batch_ingestion_job_cli
  type: io.kestra.core.tasks.scripts.Bash
  commands:
    - |
      echo "Submitting AWS Batch Job..."
      JOB_DEF_NAME="insightflow-dev-ingestion-job-def"
      JOB_QUEUE_NAME="insightflow-dev-job-queue"
      TARGET_BUCKET_NAME="insightflow-dev-raw-data"
      AWS_REGION="ap-southeast-2"

      JOB_NAME="insightflow-ingestion-{{execution.id}}"
      JOB_OUTPUT=$(aws batch submit-job \
        --region "$AWS_REGION" \
        --job-name "$JOB_NAME" \
        --job-queue "$JOB_QUEUE_NAME" \
        --job-definition "$JOB_DEF_NAME" \
        --container-overrides '{
            "environment": [
              {"name": "TARGET_BUCKET", "value": "'"$TARGET_BUCKET_NAME"'"}
            ]
          }')

      JOB_ID=$(echo "$JOB_OUTPUT" | grep -o '"jobId": "[^"]*' | awk -F'"' '{print $4}')
      echo "Submitted Job ID: $JOB_ID"
      echo '\{\{ outputs({"jobId": "'"$JOB_ID"'"}) \}\}'

# -------------------------------------
# 2. Update Glue Catalog via Crawler (using AWS CLI)
# -------------------------------------
- id: start_glue_crawler_cli
  type: io.kestra.core.tasks.scripts.Bash
  commands:
    - |
      echo "Starting AWS Glue Crawler..."
      CRAWLER_NAME="insightflow-dev-raw-data-crawler"
      AWS_REGION="ap-southeast-2"

      aws glue start-crawler --region $AWS_REGION --name "$CRAWLER_NAME"
      echo "Crawler $CRAWLER_NAME started."

  # Similar to Batch, this doesn't wait for completion. Add delay or polling if needed.

# # --- Optional Delay ---
# - id: wait_for_crawler_and_batch
#   type: io.kestra.plugin.core.flow.Pause
#   delay: PT3M # Example: Pause for 3 minutes to give Batch/Crawler time

# -------------------------------------
# 3. Run dbt Tasks (Requires dbt CLI plugin & access to dbt project files)
# -------------------------------------
- id: dbt_setup_and_run
  type: io.kestra.plugin.core.flow.WorkingDirectory
  tasks:
  - id: sync_dbt_files
    type: io.kestra.plugin.git.SyncNamespaceFiles
    url: https://github.com/pizofreude/insightflow-retail-economic-pipeline
    branch: develop
    namespace: "{{ flow.namespace }}"
    gitDirectory: dbt
    dryRun: false
    # disabled: true # Uncomment this after the first successful sync if you don't want to sync files every time

  - id: debug_synced_files
    type: io.kestra.core.tasks.scripts.Bash
    commands:
      - ls -l dbt

  - id: dbt_deps
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt deps
    namespaceFiles:
      enabled: true
    containerImage: ghcr.io/dbt-athena-community/dbt-athena:latest

  - id: dbt_seed
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt seed --target dev
    namespaceFiles:
      enabled: false  # Disable automatic file sync for this task
    profiles: |
      insightflow_dbt: # Profile name - must match profile in dbt_project.yml
        target: dev  # Tells dbt to use the 'dev' output block defined below
        outputs:
          dev:
            type: athena
            s3_staging_dir: "s3://insightflow-dev-processed-data/dbt-athena-results/"
            region_name: "ap-southeast-2"
            schema: "insightflow_dev"  # <<< ADD THIS: Target Glue DB name created by Terraform
            database: "awsdatacatalog" # <<< SET THIS: Use Glue Data Catalog alias
            threads: 4
            work_group: "primary"
    containerImage: ghcr.io/dbt-athena-community/dbt-athena:latest
        

  - id: dbt_run
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt run --target dev
    namespaceFiles:
      enabled: true
    containerImage: ghcr.io/dbt-athena-community/dbt-athena:latest

  - id: dbt_test
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt test --target dev
    namespaceFiles:
      enabled: true
    containerImage: ghcr.io/dbt-athena-community/dbt-athena:latest
    # Optional: Add dbt test results to outputs
    outputs:
      - name: dbt_test_results
        type: STRING
        defaults: "{{ task.outputs }}"
        description: "DBT test results"
    # Optional: Add dbt test results to flow outputs
    flowOutputs:
      - name: dbt_test_results
        type: STRING
        defaults: "{{ task.outputs }}"
        description: "DBT test results"

# Optional: Add triggers (e.g., schedule)
triggers:
  - id: daily_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 5 * * *"