id: insightflow_dev_pipeline # Unique ID for the flow
namespace: dev.pipelines.insightflow # Namespace to organize flows

description: |
  End-to-end pipeline for ingesting data.gov.my retail and fuel data,
  processing it with dbt, and running tests. Targets the DEV environment.

# Optional: Define inputs for the flow, e.g., target dbt environment
inputs:
  - name: dbt_target
    type: STRING
    defaults: dev

tasks:
# -------------------------------------
# 1. Ingestion via AWS Batch (using AWS CLI)
# -------------------------------------
- id: submit_batch_ingestion_job_cli
  type: io.kestra.core.tasks.scripts.Bash
  commands:
    - |
      echo "Submitting AWS Batch Job..."
      JOB_DEF_NAME="insightflow-dev-ingestion-job-def"
      JOB_QUEUE_NAME="insightflow-dev-job-queue"
      TARGET_BUCKET_NAME="insightflow-dev-raw-data"
      AWS_REGION="ap-southeast-2"

      JOB_NAME="insightflow-ingestion-{{execution.id}}"
      JOB_OUTPUT=$(aws batch submit-job \
        --region "$AWS_REGION" \
        --job-name "$JOB_NAME" \
        --job-queue "$JOB_QUEUE_NAME" \
        --job-definition "$JOB_DEF_NAME" \
        --container-overrides '{
            "environment": [
              {"name": "TARGET_BUCKET", "value": "'"$TARGET_BUCKET_NAME"'"}
            ]
          }')

      JOB_ID=$(echo "$JOB_OUTPUT" | grep -o '"jobId": "[^"]*' | awk -F'"' '{print $4}')
      echo "Submitted Job ID: $JOB_ID"
      echo '\{\{ outputs({"jobId": "'"$JOB_ID"'"}) \}\}'

# -------------------------------------
# 2. Update Glue Catalog via Crawler (using AWS CLI)
# -------------------------------------
- id: start_glue_crawler_cli
  type: io.kestra.core.tasks.scripts.Bash
  commands:
    - |
      echo "Starting AWS Glue Crawler..."
      CRAWLER_NAME="insightflow-dev-raw-data-crawler"
      AWS_REGION="ap-southeast-2"

      aws glue start-crawler --region $AWS_REGION --name "$CRAWLER_NAME"
      echo "Crawler $CRAWLER_NAME started."

  # Similar to Batch, this doesn't wait for completion. Add delay or polling if needed.

# # --- Optional Delay ---
# - id: wait_for_crawler_and_batch
#   type: io.kestra.plugin.core.flow.Pause
#   delay: PT3M # Example: Pause for 3 minutes to give Batch/Crawler time

# -------------------------------------
# 3. Run dbt Tasks (Requires dbt CLI plugin & access to dbt project files)
# -------------------------------------
- id: dbt_setup_and_run
  type: io.kestra.plugin.core.flow.WorkingDirectory
  tasks:
  - id: git_clone_dbt_project
    type: io.kestra.plugin.git.Clone
    url: https://github.com/pizofreude/insightflow-retail-economic-pipeline.git
    branch: develop
    directory: "git_clone_dbt_project"  # Use a relative path here

  - id: debug_git_clone
    type: io.kestra.core.tasks.scripts.Bash
    commands:
      - ls -l git_clone_dbt_project
      - ls -l git_clone_dbt_project/dbt

  - id: debug_working_directory
    type: io.kestra.core.tasks.scripts.Bash
    commands:
      - pwd
      - ls -l git_clone_dbt_project/dbt

  - id: dbt_deps
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt deps
    projectDir: "git_clone_dbt_project/dbt"  # Use a relative path here

  - id: dbt_seed
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt seed --target dev
    projectDir: "git_clone_dbt_project/dbt"  # Use a relative path here

  - id: dbt_run
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt run --target dev
    projectDir: "git_clone_dbt_project/dbt"  # Use a relative path here

  - id: dbt_test
    type: io.kestra.plugin.dbt.cli.DbtCLI
    commands:
      - dbt test --target dev
    projectDir: "git_clone_dbt_project/dbt"  # Use a relative path here

# Optional: Add triggers (e.g., schedule)
triggers:
  - id: daily_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 5 * * *"