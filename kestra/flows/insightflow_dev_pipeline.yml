id: insightflow_dev_pipeline # Unique ID for the flow
namespace: dev.pipelines.insightflow # Namespace to organize flows

description: |
  End-to-end pipeline for ingesting data.gov.my retail and fuel data,
  processing it with dbt, and running tests. Targets the DEV environment.

# Optional: Define inputs for the flow, e.g., target dbt environment
inputs:
  - name: dbt_target
    type: STRING
    defaults: dev

tasks:
  # -------------------------------------
  # 1. Ingestion via AWS Batch
  # -------------------------------------
  - id: submit_batch_ingestion_job
    type: io.kestra.plugin.aws.batch.SubmitJob
    accessKeyId: "{{ secret('AWS_ACCESS_KEY_ID') }}" # Use Kestra secrets for AWS creds
    secretKeyId: "{{ secret('AWS_SECRET_ACCESS_KEY') }}"
    region: "ap-southeast-2" # Or use flow input: {{ inputs.aws_region }}
    jobName: "insightflow-ingestion-{{ execution.id }}" # Unique job name per run
    jobDefinition: "{{ tf(flow.namespace,'compute').outputs.batch_job_definition_arn }}" # Fetch ARN from Terraform output (requires Kestra Terraform integration or manual input)
    # Or hardcode ARN: "arn:aws:batch:ap-southeast-2:ACCOUNT_ID:job-definition/insightflow-dev-ingestion-job-def"
    jobQueue: "{{ tf(flow.namespace,'compute').outputs.batch_job_queue_arn }}" # Fetch ARN from Terraform output
    # Or hardcode ARN: "arn:aws:batch:ap-southeast-2:ACCOUNT_ID:job-queue/insightflow-dev-job-queue"
    containerOverrides:
      environment:
        # Pass the target bucket name to the Python script
        - name: TARGET_BUCKET
          value: "{{ tf(flow.namespace,'storage').outputs.raw_s3_bucket_name }}" # Fetch from Terraform storage output
          # Or hardcode: "insightflow-dev-raw-data"
    # Optional: Specify if Kestra should wait for the job to complete
    wait: true # Default is true, wait for job completion

  # -------------------------------------
  # 2. Update Glue Catalog via Crawler
  # -------------------------------------
  - id: start_glue_crawler
    type: io.kestra.plugin.aws.glue.CrawlerStart
    accessKeyId: "{{ secret('AWS_ACCESS_KEY_ID') }}"
    secretKeyId: "{{ secret('AWS_SECRET_ACCESS_KEY') }}"
    region: "ap-southeast-2"
    name: "{{ tf(flow.namespace,'compute').outputs.glue_crawler_name }}" # Fetch from Terraform output
    # Or hardcode: "insightflow-dev-raw-data-crawler"
    # This task only starts the crawler, it doesn't wait by default.
    # We might need a separate task to check crawler status if dbt depends on immediate completion.
    # Or add a delay task, or assume crawler runs fast enough / dbt handles eventual consistency.

  # --- Optional Delay or Wait Task ---
  # Add a delay if needed to allow crawler time to finish before dbt runs
  - id: wait_for_crawler
    type: io.kestra.core.tasks.flows.Pause
    delay: PT2M # Example: Pause for 2 minutes

  # -------------------------------------
  # 3. Run dbt Tasks (Requires dbt CLI plugin & access to dbt project files)
  # -------------------------------------
  # Assumes Kestra executor has access to the dbt project files (e.g., via Git sync task)
  # and dbt CLI + adapter installed.
  - id: dbt_setup_and_run
    type: io.kestra.core.tasks.flows.WorkingDirectory # Group dbt tasks in their project dir
    tasks:
      - id: git_clone_dbt_project # Example: Clone dbt repo if not already present
        type: io.kestra.plugin.git.Clone
        url: <your-git-repo-url> # e.g., git@github.com:user/repo.git
        branch: develop # Checkout the dev branch for the dev pipeline
        # Add credentials if needed

      - id: dbt_deps
        type: io.kestra.plugin.dbt.cli.DbtCLI
        # runner: PROCESS # Or DOCKER if running dbt in its own container
        commands:
          - dbt deps
        # Optional: Specify dbt project directory if not current dir
        # projectDir: /path/to/dbt/project # Usually '.' if WorkingDirectory task is used

      - id: dbt_seed
        type: io.kestra.plugin.dbt.cli.DbtCLI
        commands:
          - dbt seed --target dev # Specify the dbt target

      - id: dbt_run
        type: io.kestra.plugin.dbt.cli.DbtCLI
        commands:
          - dbt run --target dev

      - id: dbt_test
        type: io.kestra.plugin.dbt.cli.DbtCLI
        commands:
          - dbt test --target dev

    # Define the working directory relative to Kestra's internal storage or execution root
    # This path needs to contain the dbt project after the git clone step
    workingDirectory: "{{ outputs.git_clone_dbt_project.outputFiles.workingDir }}/dbt" # Example path

# Optional: Add triggers (e.g., schedule)
triggers:
  - id: daily_schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: "0 5 * * *" # Example: Run daily at 5 AM UTC

# Optional: Add error handling tasks
# errors:
#   - id: send_failure_notification
#     type: io.kestra.plugin.notifications.slack.SlackExecution # Example
#     url: "{{ secret('SLACK_WEBHOOK_URL') }}"
#     payload: |
#       {
#         "text": "Kestra Flow Failed!\n*Flow:* {{ flow.namespace }}.{{ flow.id }}\n*Execution:* {{ execution.id }}\n*Link:* {{ KESTRA_UI_URL }}/ui/executions/{{ execution.id }}"
#       }

