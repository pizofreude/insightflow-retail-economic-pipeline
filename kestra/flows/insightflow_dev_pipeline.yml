id: insightflow_dev_pipeline # Unique ID for the flow
namespace: dev.pipelines.insightflow # Namespace to organize flows

description: |
  End-to-end pipeline for ingesting data.gov.my retail and fuel data,
  processing it with dbt, and running tests. Targets the DEV environment.

# Optional: Define inputs for the flow, e.g., target dbt environment
inputs:
  - name: dbt_target
    type: STRING
    defaults: dev

tasks:
# -------------------------------------
# 1. Ingestion via AWS Batch (using AWS CLI)
# -------------------------------------
- id: submit_batch_ingestion_job_cli
  type: io.kestra.core.tasks.scripts.Bash
  # AWS CLI needs to be available in the execution environment (likely present in -full image)
  # Authentication relies on the EC2 instance role
  commands:
    - |
      echo "Submitting AWS Batch Job..."
      # Fetch resource names/ARNs from Terraform outputs (replace placeholders)
      JOB_DEF_NAME="insightflow-dev-ingestion-job-def" # Or use ARN: {{ tf(flow.namespace,'compute').outputs.batch_job_definition_arn }}
      JOB_QUEUE_NAME="insightflow-dev-job-queue"       # Or use ARN: {{ tf(flow.namespace,'compute').outputs.batch_job_queue_arn }}
      TARGET_BUCKET_NAME="insightflow-dev-raw-data" # Or use TF output: {{ tf(flow.namespace,'storage').outputs.raw_s3_bucket_name }}
      AWS_REGION="ap-southeast-2" # Or use flow input: {{ inputs.aws_region }}

      # Submit job and capture output (contains jobId)
      # Note the JSON structure for containerOverrides. Environment values must be strings.
      JOB_OUTPUT=$(aws batch submit-job \
        --region $AWS_REGION \
        --job-name "insightflow-ingestion-{{ execution.id }}" \
        --job-queue "$JOB_QUEUE_NAME" \
        --job-definition "$JOB_DEF_NAME" \
        --container-overrides '{
            "environment": [
              {"name": "TARGET_BUCKET", "value": "'"$TARGET_BUCKET_NAME"'"}
            ]
          }')

      # Extract Job ID (optional, but useful for waiting/checking status)
      JOB_ID=$(echo $JOB_OUTPUT | jq -r .jobId)
      echo "Submitted Job ID: $JOB_ID"
      # Output the Job ID for potential use in later tasks
      echo '{{ outputs({"jobId": "'"$JOB_ID"'"}) }}'

  # If you need to wait for the Batch job, you'd add a subsequent task
  # using Bash/AWS CLI to poll `aws batch describe-jobs --jobs $JOB_ID`
  # until status is SUCCEEDED or FAILED. This adds complexity.
  # For now, we'll assume it runs reasonably fast or subsequent tasks handle delays.

  # -------------------------------------
  # 2. Update Glue Catalog via Crawler (using AWS CLI)
  # -------------------------------------
  - id: start_glue_crawler_cli
    type: io.kestra.core.tasks.scripts.Bash
    commands:
      - |
        echo "Starting AWS Glue Crawler..."
        CRAWLER_NAME="insightflow-dev-raw-data-crawler" # Or use TF output: {{ tf(flow.namespace,'compute').outputs.glue_crawler_name }}
        AWS_REGION="ap-southeast-2" # Or use flow input

        aws glue start-crawler-run --region $AWS_REGION --name "$CRAWLER_NAME"
        echo "Crawler $CRAWLER_NAME started."

    # Similar to Batch, this doesn't wait for completion. Add delay or polling if needed.

  # --- Optional Delay ---
  - id: wait_for_crawler_and_batch
    type: io.kestra.core.tasks.flows.Pause
    delay: PT3M # Example: Pause for 3 minutes to give Batch/Crawler time


  # -------------------------------------
  # 3. Run dbt Tasks (Requires dbt CLI plugin & access to dbt project files)
  # -------------------------------------
  # Assumes Kestra executor has access to the dbt project files (e.g., via Git sync task)
  # and dbt CLI + adapter installed.
  - id: dbt_setup_and_run
    type: io.kestra.core.tasks.flows.WorkingDirectory # Group dbt tasks in their project dir
    tasks:
      - id: git_clone_dbt_project # Example: Clone dbt repo if not already present
        type: io.kestra.plugin.git.Clone
        url: https://github.com/pizofreude/insightflow-retail-economic-pipeline.git # e.g., git@github.com:user/repo.git
        branch: develop # Checkout the dev branch for the dev pipeline
        # Add credentials if needed

      - id: dbt_deps
        type: io.kestra.plugin.dbt.cli.DbtCLI
        # runner: PROCESS # Or DOCKER if running dbt in its own container
        commands:
          - dbt deps
        # Optional: Specify dbt project directory if not current dir
        # projectDir: /path/to/dbt/project # Usually '.' if WorkingDirectory task is used

      - id: dbt_seed
        type: io.kestra.plugin.dbt.cli.DbtCLI
        commands:
          - dbt seed --target dev # Specify the dbt target

      - id: dbt_run
        type: io.kestra.plugin.dbt.cli.DbtCLI
        commands:
          - dbt run --target dev

      - id: dbt_test
        type: io.kestra.plugin.dbt.cli.DbtCLI
        commands:
          - dbt test --target dev

    # Define the working directory relative to Kestra's internal storage or execution root
    # This path needs to contain the dbt project after the git clone step
    workingDirectory: "{{ outputs.git_clone_dbt_project.outputFiles.workingDir }}/dbt" # Example path for dbt_project.yml in subdir dbt/

# Optional: Add triggers (e.g., schedule)
triggers:
  - id: daily_schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: "0 5 * * *" # Example: Run daily at 5 AM UTC

# Optional: Add error handling tasks
# errors:
#   - id: send_failure_notification
#     type: io.kestra.plugin.notifications.slack.SlackExecution # Example
#     url: "{{ secret('SLACK_WEBHOOK_URL') }}"
#     payload: |
#       {
#         "text": "Kestra Flow Failed!\n*Flow:* {{ flow.namespace }}.{{ flow.id }}\n*Execution:* {{ execution.id }}\n*Link:* {{ KESTRA_UI_URL }}/ui/executions/{{ execution.id }}"
#       }

