id: insightflow_dev_pipeline # Unique ID for the flow
namespace: dev.pipelines.insightflow # Namespace to organize flows

description: |
  End-to-end pipeline for ingesting data.gov.my retail and fuel data,
  processing it with dbt, and running tests. Targets the DEV environment.

# Optional: Define inputs for the flow, e.g., target dbt environment
inputs:
  - name: dbt_target
    type: STRING
    defaults: dev

tasks:
# -------------------------------------
# 1. Ingestion via AWS Batch (using AWS CLI)
# -------------------------------------
- id: submit_batch_ingestion_job_cli
  type: io.kestra.core.tasks.scripts.Bash
  commands:
    - |
      echo "Submitting AWS Batch Job..."
      # Fetch resource names/ARNs from Terraform outputs (replace placeholders)
      JOB_DEF_NAME="insightflow-dev-ingestion-job-def" # Or use ARN: \{\{ tf(flow.namespace,'compute').outputs.batch_job_definition_arn \}\}
      JOB_QUEUE_NAME="insightflow-dev-job-queue"       # Or use ARN: \{\{ tf(flow.namespace,'compute').outputs.batch_job_queue_arn \}\}
      TARGET_BUCKET_NAME="insightflow-dev-raw-data"    # Or use TF output: \{\{ tf(flow.namespace,'storage').outputs.raw_s3_bucket_name \}\}
      AWS_REGION="ap-southeast-2"                      # Or use flow input: \{\{ inputs.aws_region \}\}

      # Submit job and capture output (contains jobId)
      # Note the JSON structure for containerOverrides. Environment values must be strings.
      JOB_OUTPUT=$(aws batch submit-job \
        --region "$AWS_REGION" \
        --job-name "insightflow-ingestion-\{\{ execution.id \}\}" \
        --job-queue "$JOB_QUEUE_NAME" \
        --job-definition "$JOB_DEF_NAME" \
        --container-overrides '{
            "environment": [
              {"name": "TARGET_BUCKET", "value": "'"$TARGET_BUCKET_NAME"'"}
            ]
          }')

      # Extract Job ID (optional, but useful for waiting/checking status)
      JOB_ID=$(echo "$JOB_OUTPUT" | jq -r .jobId)
      echo "Submitted Job ID: $JOB_ID"
      # Output the Job ID for potential use in later tasks
      echo '\{\{ outputs({"jobId": "'"$JOB_ID"'"}) \}\}'

  # If you need to wait for the Batch job, you'd add a subsequent task
  # using Bash/AWS CLI to poll `aws batch describe-jobs --jobs $JOB_ID`
  # until status is SUCCEEDED or FAILED. This adds complexity.
  # For now, we'll assume it runs reasonably fast or subsequent tasks handle delays.

# -------------------------------------
# 2. Update Glue Catalog via Crawler (using AWS CLI)
# -------------------------------------
- id: start_glue_crawler_cli
  type: io.kestra.core.tasks.scripts.Bash
  commands:
    - |
      echo "Starting AWS Glue Crawler..."
      CRAWLER_NAME="insightflow-dev-raw-data-crawler" # Or use TF output: {{ tf(flow.namespace,'compute').outputs.glue_crawler_name }}
      AWS_REGION="ap-southeast-2" # Or use flow input

      aws glue start-crawler-run --region $AWS_REGION --name "$CRAWLER_NAME"
      echo "Crawler $CRAWLER_NAME started."

  # Similar to Batch, this doesn't wait for completion. Add delay or polling if needed.

# --- Optional Delay ---
- id: wait_for_crawler_and_batch
  type: io.kestra.plugin.core.flow.Pause
  delay: PT3M # Example: Pause for 3 minutes to give Batch/Crawler time


# -------------------------------------
# 3. Run dbt Tasks (Requires dbt CLI plugin & access to dbt project files)
# -------------------------------------
- id: dbt_setup_and_run
  type: io.kestra.plugin.core.flow.WorkingDirectory
  tasks:
    - id: git_clone_dbt_project # Clone the dbt repository
      type: io.kestra.plugin.git.Clone
      url: https://github.com/pizofreude/insightflow-retail-economic-pipeline.git
      branch: develop

    - id: dbt_deps
      type: io.kestra.plugin.dbt.cli.DbtCLI
      commands:
        - dbt deps
      # Specify the dbt project directory relative to the cloned repository
      projectDir: "{{ outputs.git_clone_dbt_project.outputFiles }}/dbt"

    - id: dbt_seed
      type: io.kestra.plugin.dbt.cli.DbtCLI
      commands:
        - dbt seed --target dev
      projectDir: "{{ outputs.git_clone_dbt_project.outputFiles }}/dbt"

    - id: dbt_run
      type: io.kestra.plugin.dbt.cli.DbtCLI
      commands:
        - dbt run --target dev
      projectDir: "{{ outputs.git_clone_dbt_project.outputFiles }}/dbt"

    - id: dbt_test
      type: io.kestra.plugin.dbt.cli.DbtCLI
      commands:
        - dbt test --target dev
      projectDir: "{{ outputs.git_clone_dbt_project.outputFiles }}/dbt"

# Optional: Add triggers (e.g., schedule)
triggers:
  - id: daily_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 5 * * *" # Example: Run daily at 5 AM UTC

# Optional: Add error handling tasks
# errors:
#   - id: send_failure_notification
#     type: io.kestra.plugin.notifications.slack.SlackExecution # Example
#     url: "{{ secret('SLACK_WEBHOOK_URL') }}"
#     payload: |
#       {
#         "text": "Kestra Flow Failed!\n*Flow:* {{ flow.namespace }}.{{ flow.id }}\n*Execution:* {{ execution.id }}\n*Link:* {{ KESTRA_UI_URL }}/ui/executions/{{ execution.id }}"
#       }

